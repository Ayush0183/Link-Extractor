{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures #for managing concurrent tasks\n",
        "import queue #creating a queue for the data\n",
        "import requests #for making web requests\n",
        "from bs4 import BeautifulSoup #using the BeautifulSoup library to extract hyperlinks (links) from the HTML\n",
        "\n",
        "# Constants\n",
        "Producers = 2\n",
        "Consumers = 4\n",
        "max_queue = 50\n",
        "\n",
        "# Shared queue for storing HTML content\n",
        "html_queue = queue.Queue(maxsize=max_queue)\n",
        "\n",
        "# Function to fetch HTML content from a URL\n",
        "def fetch(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to parse HTML content and extract hyperlinks\n",
        "def parse_html(html, url):\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        links = [a.get('href') for a in soup.find_all('a')]\n",
        "        return links\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Producer function\n",
        "def producer(url_list):\n",
        "    for url in url_list:\n",
        "        html = fetch(url)\n",
        "        if html:\n",
        "            html_queue.put((html, url))\n",
        "\n",
        "# Consumer function\n",
        "def consumer():\n",
        "    while True:\n",
        "        try:\n",
        "            html, url = html_queue.get(timeout=5)  # Adjust the timeout as needed\n",
        "            links = parse_html(html, url)\n",
        "            print(f\"Links found in {url}: {links}\")\n",
        "        except queue.Empty:\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    urls = []  # Replace with your list of URLs\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=Producers) as producer_executor:\n",
        "        producer_executor.map(producer, [urls])\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=Consumers) as consumer_executor:\n",
        "        for _ in range(Consumers):\n",
        "            consumer_executor.submit(consumer)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "GhSt_7Bt-ZrR"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}